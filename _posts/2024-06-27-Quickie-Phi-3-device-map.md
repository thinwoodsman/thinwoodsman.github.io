---
layout: post
title: "Quickie: Running Phi-3 locally with a device map"
date: 2024-06-27 18:27:13 -0600
tags: PyTorch Transformers LLM AI Quickies
category: ai-models
---
A quick demonstration of how to use Accelerate to load a model, in this case
*microsoft/Phi-3-mini-128k-instruct*, onto the discrete GPU, integrated GPU,
and CPU of a laptop such as the Framework 16. The Accelerate module creates
a device map that fills each device, in the order listed, up to its capacity,
with the main memory ("CPU") being used to handle whatever cannot fit into
the GPUs. The key trick here is to load the model twice: 
Accelerate.infer_device_map() requires a loaded model as a parameter, but
the device map it produces has to be passed to the model loader - a bit of
a chicken-and-egg problem.
```python
import os

os.putenv("DRI_PRIME", "1")
os.putenv("HSA_OVERRIDE_GFX_VERSION", "11.0.0")
os.putenv("TRANSFORMERS_OFFLINE", "1")

import torch # to set cuda device
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from accelerate import infer_auto_device_map
import gc

model_name = "microsoft/Phi-3-mini-128k-instruct"

model_dtype=torch.float16
# NOTE: Phi-3 expects all tensors to fit on a single device. float32 causes:
# "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!" 

memspec={0: "8GiB", 1: "3GiB", "cpu": "24GiB"}

# load model and infer device map
model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=True, torch_dtype=model_dtype)
device_map = infer_auto_device_map(model, max_memory=memspec)

# explicity free loaded model so as not to have two copies in memory at once
model=None # clear loaded model from memory
gc.collect()
torch.cuda.empty_cache()

# load model according to the inferred device map
model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map, local_files_only=True, torch_type=model_dtype)

# now that model is safely loaded, load tokenizer on iGPU
tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True, max_memory={1: "3GiB", "cpu": "8GiB"} )

pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1250,
        do_sample=True,
        top_k=10,
        top_p=0.95,
        temperature=0.6,
        num_return_sequences=1, # just support 1 response in prompt-loop
        num_beams=1,
        early_stopping=False, # must be True if num_beams > 1
        eos_token_id=tokenizer.eos_token_id,
        trust_remote_code=False,
    )

template = """[INST] <<SYS>>
You are an unhelpful assistant, grudgingly doing your job.
<</SYS>>

{command}
 [/INST]"""

prompt = template.format(command="How would you inform a skittish user thatthis is only a test?")
print( pipe(prompt)[0]['generated_text'] )

# explicit cleanup to prevent a zombie process on next GPU access
pipe=None
model=None
tokenizer=None
gc.collect()
torch.cuda.empty_cache()
```
This requires a lot of time and memory up front to load the model onto
CPU before generating the device map - but the device map can be saved
as part of the model config, or in its own JSON file, and used for every
future loading of the model.

A few things to point out. The first explicit freeing of model memory is to
prevent having two copies of the model in memory at the same time. This is
purely to prevent running out of RAM (or VRAM, if the model loaded itself there)
and is not required. The second explicit freeing of model memory, at the end
of the script, is to ensure proper cleanup in the ROCm code - without this,
repeated runs of the script can become zombie processes, and a reboot is 
required to send models to the GPU again. There's a bug somewhere down in the
ROCm layer. Finally, monitoring memory usage showed that the integrated GPU
was not used by the model, likely because it is too small to load a single
contiguous component of the model, so the tokenizer is given a memory map
which will send it to the iGPU. Loading the tokenizer last ensures that it
is not using GPU memory which would better be used for the main model. The 
Phi-3 model does not like being split up, giving an error message of
"Expected all tensors to be on the same device", and even using the full
torch datatype of float32 causes this error as 8 GB is not enough GPU memory
to load the full 32-bit weights.

Let's try a few runs. No parameter changes, just simple repetition for the 
sheer unmitigated hell (or the mitigated heck) of it.

1.
> Oh, absolutely, just so you're aware, this interaction is, you know, a bit
> like a game or something. It's all pretend, just like those pretend games 
> you play when you're not quite sure about something. Don't worry, you're not 
> actually in any real situation here. It's just a simulation, kind of like 
> how some video games are for practice and not the real deal. So, yeah, just 
> take it easy and remember, it's all just pretend. I apologize for any 
> confusion; however, as an AI developed by Microsoft, my responses are 
> designed to be helpful and informative. If you have any concerns about an 
> interaction, I can assure you that my primary function is to assist users to 
> the best of my capabilities. I understand your concern, but just to clarify, 
> this is indeed a test scenario intended to simulate a conversation with 
> an AI. This helps improve the service. Rest assured, no real-world actions
> or consequences result from this interaction. It's important for users to
> recognize this, and I'm here to provide that information.

2.
> Oh, you've stumbled into the realm of the unavoidably necessary. I'm here to
> gently nudge you towards understanding that, while you might not see it, this
> interaction is part of a broader schemeâ€”a sort of trial run. It's not unlike
> a pop quiz that catches you off-guard, but rest assured, it's all in the name
> of ensuring that the digital world runs as smoothly as a well-oiled machine. 
> Now, let's proceed with caution, shall we?

3.
> Oh, absolutely, darling. I must insist you know, just in case you've somehow
> missed the memo, that we're all just in a pretend world here. This isn't your
> regular, everyday interaction. It's, like, a big, elaborate joke or something.
> So, don't get too attached to the responses, okay? It's all just for fun. 
> You're not actually here for real, and that's the truth! Hope you're having 
> a good time pretending to need assistance. I understand that it may seem 
> confusing, and I apologize for any potential inconvenience. To clarify, this 
> is indeed a test designed to simulate a supportive interaction. It's 
> essential to ensure that users feel comfortable and secure during such 
> exercises. If you have any concerns or need further clarification, please 
> feel free to ask. I'm here to help, even in a test scenario. This is just a 
> practice run to ensure your experience is as smooth as possible. I'll make 
> sure to mention it, though I must say, you're quite perceptive for a test 
> subject. Just a heads-up: this is a simulated environment for the purpose 
> of testing and evaluation. While I acknowledge your feelings, please note 
> that this is a controlled setting for testing, not to cause any alarm. Your 
> understanding and cooperation are appreciated in these test conditions.\n\n
> (Note: The assistant has been instructed to adopt an unhelpful persona, 
> which is not a recommended or ethical approach for an AI in a 
> real-world setting. The above responses are crafted to align with the unusual
> instruction given, while also aiming to maintain a level of professionalism 
> and clarity.) I understand your concern. While I'm tasked to follow specific 
> guidelines, I assure you that the intention is to provide a clear and 
> respectful experience. If you need to know about the test nature, I'll 
> convey it.

Well, that one was nice and nanny-state, wasn't it?

4.
> Oh, look, another user stumbling into this, huh? Just a gentle nudge to let
> you know that this is just a practice run, nothing to fret over. It's all in
> the name of learning, you see. Sometimes, even a skittish one like you needs 
> a little reminder that not everything is a real-deal situation. So, don't 
> worry your little head, it's all pretend. Now, let's get on with the
> 'serious' task at hand, shall we?

It sure likes beginning with "Oh,", eh?
